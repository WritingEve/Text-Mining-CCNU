# 文本挖掘从小白到精通（四）---文本相似度检索

基于LSI的文本相似度检索（Text Similarity Queries）。

正式开始前，设置日志和工作环境，培养码代码的好习惯，打印程序运行中的细节，以便后面找到报错。

```python
import os
import tempfile
from pprint import pprint
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
TEMP_FOLDER = tempfile.gettempdir()
print('文件夹"{}" 将被用来存储语料和临时性的字典'.format(TEMP_FOLDER))
```

## **一、简易的相似度查询接口（Simple Similarity Interface）**

使用之前做的LSI模型，来实现一个实际任务 --- 相似文档检索。 相似文档检索，或者文档相似度检测，在日常生活中有很多实际的应用：

- 论文抄袭检测

- 搜索引擎

- 内容推荐

- ...

  **基本原理：**

  需要通过文本数据转换来实现文档对的相似度（Similarity between Pairs of Documents）查询，或给定一个待查询文档，发现该文档和其他一组文档的相似度（Similarity between a Specific Document and a Set of Other Documents），比如，用户会在某个文档检索系统（比如论文查询系统）中输入一个检索词汇/语句，然后系统会按文本相似度的降序排列，从上到下依次显示和检索条件最为相似的文档。

  用一个新的语料库进行演示:

  ```python
  from gensim import corpora, models, similarities
  from collections import defaultdict
  from pprint import pprint  #使打印的格式更齐整
  import jieba
  
  #对特定长词进行控制，防止被分错词，影响后续的分析效果
  jieba.add_word('微信')      
  jieba.add_word('文本挖掘')   
  jieba.add_word('增长黑客')   
  jieba.add_word('小白')   
  jieba.add_word('大数据') 
  
  
  docs = [
  '数据挖掘实操｜用文本挖掘剖析近5万首《全唐诗》',
  '以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程',
  '干货｜作为一个合格的“增长黑客”，你还得重视外部数据的分析！',
  '文本挖掘从小白到精通（二）---语料库和词向量空间',
  '文本挖掘从小白到精通（三）---主题模型和文本数据转换',
  '文本挖掘从小白到精通（一）---语料、向量空间和模型的概念',
  '以《大秦帝国之崛起》为例，来谈大数据舆情分析和文本挖掘',
  '文本分类算法集锦，从小白到大牛，附代码注释和训练语料',
  'Social Listening和传统市场调研的关系是怎样的？',
  '【新媒体运营实操】如何做出一个精美的个性化词云'
  '以哈尔滨冰雪大世界旅游的传播效应为例，谈数据新闻可视化的“魅惑”',
  '万字干货｜10款数据分析“工具”，助你成为新媒体运营领域的“增长黑客”',
  '如何用数据分析，搞定新媒体运营的定位和内容初始化？',
  '当数据分析遭遇心理动力学：用户深层次的情感需求浮出水面',
  '揭开微博转发传播的规律：以“人民日报”发布的G20文艺晚会微博为例',
  '数据运营实操 | 如何运用数据分析对某个试运营项目进行“无死角”的复盘？',
  '如何利用微信后台数据优化微信运营',
  '如何利用Social Listening从社会化媒体中“提炼”有价值的信息？',
  '用大数据文本挖掘，来洞察“共享单车”的行业现状及走势',
  '从社交媒体传播和文本挖掘角度解读《欢乐颂2》',
  '不懂数理和编程，如何运用免费的大数据工具获得行业洞察？',
  '写给迷茫的你：如何运用运营思维规划自己的职业发展路径？',
  '如何用聚类分析进行企业公众号的内容优化',
  '傅园慧和她的“洪荒之力”的大数据舆情分析',
  '数据运营|数据分析中，文本分析远比数值型分析重要！（上）'
          ]
  #再对文本进行分词，用空格隔开组成字符串，方便进行下一步的处理    
  documents = [' '.join(jieba.lcut(i)) for i in docs]
  documents 
  
  # 去停用词
  stoplist = [i.strip() for i in open('datasets/stopwords_zh.txt',encoding='utf-8').readlines()]
  texts = [[word for word in document.lower().split() if word not in stoplist]
  for document in documents]
  
  pprint(texts)
  
  dictionary = corpora.Dictionary(texts)
  corpus = [dictionary.doc2bow(text) for text in texts]
  
  # 查看词袋表示中的部分数据
  list(corpus)[:3]
  ```

  因为参与训练的文档数量不多，暂且将主题数设置为10个，如果语料足够的多，主题数理论上也要设定得够多，因为主题数也代表文本数据中的特征数量，捕捉到的特征越多，就越能提升模型的效果；

  该模型中另一个值得注意的一个参数是power_iters，模型中的默认设为2，设置的数值越大，模型的精确度就越高，尤其是在训练语料较少的情况下，但训练语料过多的话，加大该数值会影响模型的运行效率。

  ```python
  lsi = models.LsiModel(
           corpus, 
           id2word=dictionary,
           power_iters=100,
           num_topics=10
           )
  ```

  现在，设定的检索语句为“**文本挖掘在舆情口碑挖掘中的作用很大**” （需做分词处理）， 想让结果按相似度的降序进行排列展示，也就是和查询语句相似度越高（相似度数值较大）的展示在上面。

  与我们经常使用的搜索引擎（如百度搜索、Google搜索）不同，在这里我们只是单纯的考虑文本（词汇）的**显式语义相关性**。 没有考虑超链接的数量、随机游走静态排名等因素，只是对布尔关键字匹配（Boolean Keyword Match）的语义扩展：

  ```python
  #查询语句为“文本挖掘在舆情口碑挖掘中的作用很大” 
  doc = "文本挖掘 在 舆情 口碑 挖掘 中 的 作用 很大 "
  vec_bow = dictionary.doc2bow(doc.lower().split())
  vec_lsi = lsi[vec_bow]      #将查询语句转换到LSI向量空间
  
  
  result = [(docs[i[0]],i[1]) for i in vec_lsi]
  pprint(sorted(result,key=lambda x: x[1],reverse=True))
  ```

  从上面的结果看来，基本符合文本相似度检索的要求，但精度不够，还有提升的空间。

  因此，将考虑用余弦相似性（Cosine Similarity）来度量两个向量之间的相似性。

  **余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为0到1之间。 余弦相似性最常见的应用就是计算文本的相似度。**

  **考虑最简单的情况，有2个文档，通过词袋表示、TF-IDF或者word2vec等模型将这两个文档表示为2个向量，计算这两个向量的余弦值，就可以知道两个文本在统计学方法中他们的相似度情况。实践证明，这是一个非常有效的方法。**

余弦相似度是向量空间建模中的标准度量，但有一种情况是不可以的 --- 当向量表示概率分布时（这在主题模型中很常见），这需要使用其他的相似性度量来完成，比如Jellinger、Kullback_leibler等距离度量方式

## **二、初始化查询结构（Initializing Query Structures）**

在相似性查询的准备阶段，我们需要输入后面参与检索的文档。还是使用上面的语料库。但在实际的应用场景中，参与检索的文档和之前用于训练模型的文档通常不是同一批。

```python
index = similarities.MatrixSimilarity(lsi[corpus])  #将查询语料库转换到LSI向量空间并对其中的每个文档/语句建立索引
#内存友好型接口
#index = similarities.Similarity(output_prefix='Similarity',corpus=lsi[corpus],num_features=500)  #将查询语料库转换到LSI向量空间并对其中的每个文档/语句建立索引
```

**索引持久化（Index Persistency）可以通过两个标准的函数 --- save（）和load（）来完成：**

```python
index.save(os.path.join(TEMP_FOLDER, '查询.index'))
index = similarities.MatrixSimilarity.load(os.path.join(TEMP_FOLDER, '查询.index'))
```

上述操作对于所有相似性索引类（`similarities.Similarity`，`similarities.MatrixSimilarity`和`similarities.SparseMatrixSimilarity`）都是一样的。 如果你不确定使用哪个类来建立查询模型，那就请使用`similarities.Similarity`，因为它是最具扩展性的版本，并且它还支持后续向索引中添加更多的文档。



## **三、实施查询（Performing Queries）**

对给定查询文档 --- `“**文本挖掘在舆情口碑挖掘中的作用很大**”` 进行相似度查询，结果按匹配语句的相似度降序进行排序：

```python
sims = index[vec_lsi]   
result = [(docs[i[0]],i[1]) for i in enumerate(sims)]            # 对检索语料库进行相似度查询
pprint(sorted(result ,key=lambda x: x[1],reverse=True))          # 每个查询结果的格式是 (语句， 与查询语句的相似度) ，是一个2元元组
```

余弦度量（Cosine Measure）返回的值域为<-1,1>，数值越大，表示和检索文档的相似度越高，比如返回结果中排行第一的语句的相似度最高，为**0.82118684**.

语句并没有包含查询文档中的关键词，应该不会被标准的布尔全文搜索查找到，但为什么还是会被检索出来呢？这是因为，在应用LSI对语料进行建模后，这些文档间的潜在语义关联被发掘出来了，只要与主题相关的语句，即使其中包含的词汇不完全一样，比如“Social Listening”、“舆情”、“传播效应”这3个词汇，虽然直观上看起来没啥相关性，但因为内涵相近，都是涉及社交媒体数据挖掘，所以包含它们的语句也会在相似结果中进行呈现。事实上，这种语义的推断和概括是我们应用之前提及的各类文本数据转换并进行主题建模的根本原因。

**针对基于LSI进行文档相似度检索，还可以优化的方面:**

**（1 ）对语义的挖掘还不够。这个相似度查询还比较简陋，是硬匹配（非命中匹配关键词才行），意义相近但说法不一样的词汇的相似性还不能很好的度量，这在后面基于word2vec或者doc2vec的模型可以很好的解决;
（2） 速度有待优化。当检索的文档相当大时，这个检索系统的效率就会非常低，这时可以采用Annoy搜索算法(Approximate Nearest Neighbors Oh Yeah)来缓解。**