# 文本挖掘从小白到精通（三）---主题模型和文本数据转换

主要提及主题模型和文本数据转换（Topics and Text Data Transformation），这是在上篇文章涉及的词袋模型（Bag-of-Words）的基础上做的进一步文本特征提取。

正式开始前，设置日志，培养码代码的好习惯，打印程序运行中的细节，以便后面找到报错。

```python
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
```

工作路径查询：

```python
import tempfile
import os.path


TEMP_FOLDER = tempfile.gettempdir()
print('文件夹"{}" 将被用来存储语料和临时性的字典'.format(TEMP_FOLDER))
```



## **1. 数据转换接口（Transformation interface）**

以向量流的形式创建了一个文档语料库。 继续使用该语料库：

```python
from gensim import corpora, models, similarities
if os.path.isfile(os.path.join(TEMP_FOLDER, 'deerwester.dict')):
    dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))
    corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm'))
    print("使用前面教程中产生的语料文件。")
else:
    print("请运行前面的教程，以生成语料文件。")
```

```python
#生成语料库
import jieba
jieba.add_word('知识图谱')
raw_corpus = ['商业新知:知识图谱为内核,构建商业创新服务完整生态。',
'如何更好利用知识图谱技术做反欺诈? 360金融首席数据科学家沈赟开讲。',
'知识管理 | 基于知识图谱的国际知识管理领域可视化分析。',
'一文详解达观数据知识图谱技术与应用。',
'知识图谱技术落地金融行业的关键四步。',
'一文读懂知识图谱的商业应用进程及技术背景。',
'海云数据CPO王斌:打造大数据可视分析与AI应用的高科技企业。',
'智能产业|《人工智能标准化白皮书2018》带来创新创业新技术标准。',
'国家语委重大科研项目“中华经典诗词知识图谱构建技术研究”开题。',
'最全知识图谱介绍:关键技术、开放数据集、应用案例汇总。',
'中译语通Jove Mind知识图谱平台 引领企业智能化发展。',
'知识图谱:知识图谱赋能企业数字化转型，为企业升级转型注入新能量。']

raw_corpus = [' '.join(jieba.lcut(i)) for i in raw_corpus]

stoplist = [i.strip() for i in open('datasets/stopwords_zh.txt',encoding='utf-8').readlines()]

texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in raw_corpus]
         
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1
        
processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]

dictionary = corpora.Dictionary(processed_corpus)
dictionary.token2id
```

```python
#查看保存字典（Dictionary）中的前3个词汇：
print(dictionary[0])
print(dictionary[1])
print(dictionary[2])
```

展示**如何将文档从一个向量表示（Vector Representation ）是转换为另一个向量表示**。 在此过程中，会涉及到两个目标：

- **挖掘语料库中潜藏的结构，发现词汇之间的关联性。**前一篇文章中提到的词袋模型/表示，并不能很好的挖掘语料中的**词序特征**（语句中词汇的先后顺序）和**语义**特征（上下文、词性搭配等），进行文本数据转换后则可以一种更加反映语义相关性的方式来表征文档，以提高后续文本聚类、文本分类等高阶文本挖掘应用的效果。
- **使文档表示更紧凑。** 这既提高了**效率**（新的文档表示会消耗更少的计算资源，转换过程会更快），也提升了**效果**（边际数据的趋势被忽略，也就是语料中的超低频和超高频词汇对语料的影响被削弱，从而达到数据降噪的目的，更能抓住文本中的重要特征）。

### **1.1 创建转换（Creating a Transformation）**

*** step 1 -- 初始化模型:***

 ```python
 tfidf = models.TfidfModel(corpus)   
 ```

* 注：不同的转换可能需要不同的初始化参数 --- 在TF-IDF的情境下，“训练”仅包括通过一次性提供语料库并计算其所有特征的文档频率，模型不需要额外设置参数。 训练其他文本转换模型，比如潜在语义分析模型（Latent Semantic Analysis，LSI）或隐含狄利克雷分布模型（Latent Dirichlet Allocation），可能复杂很多，会涉及更多的参数设置，同时也需要更多的时间进行训练。

***（文本数据）转换总是在两个特定的向量空间（Two Specific Vector Spaces）之间进行。 必须使用相同的向量空间（即同一组特征id）进行模型训练和后续的向量转换。 如果不能使用相同的输入特征空间（Input Feature Space），例如使用不同的字符串进行预处理，或者使用不同的特征ID，抑或本应使用TF-IDF向量却采用词袋表示来输入向量，这些“误操作”将产生转换期间的特征不匹配，进而导致错误的运行结果或产生异常。***

***step 2 -- 使用模型对词袋表示向量进行转换：***

```python
doc_bow = [(0, 3), (1, 5)]
print(tfidf[doc_bow]) 

#或者，直接对整个（训练）语料库进行特征转换：
corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
	print(doc)
```

**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。**

比如，示例语料库中的第一句话 --- '**商业新知:知识图谱为内核,构建商业创新服务完整生态。**'，它原先可由词袋模型表征为

[(0, 1), (1, 2), (2, 1)] 

经过TF-IDF转换模型转换后则变为：

```
[(0, 0.4467512594633994), (1, 0.8935025189267988), (2, 0.045459441696222264)]
```

这里0，1，2在字典中分别对应‘创新'、'商业'、'知识图谱' 这3个词汇，根据这3个词的权重大小可知，对于这句话，最重要（最能代表这句话含义）的词汇是“商业”，其次是“创新”，最次是“知识图谱”。这很符合直觉：这12句话中都包含“知识图谱”，因此该词对各个文档/语句的代表性很弱。

*** 调用`model[corpus]`在已有的语料库文档流上创建一个包装器（Wrapper） --- 实际的文本数据转换在文档迭代期间即时完成（Done on-the-Fly）。 我们无法在调用`corpus_transformed = model [corpus]`时转换整个语料库 --- 因为这意味着结果将会存储在本地内存中，这与gensim的内存独立（Memory-Independence）的目标相悖。 如果你将多次迭代待转换的corpus_transformed，且转换代价较高，请先将生成的语料库序列化（Serialize）到本地，以便后续使用。***

***step 3 --- 基于前面的特征，进行LSI模型构建*** 

 ```python
 lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3) # 初始化 LSI 转换
 corpus_lsi = lsi[corpus_tfidf] #在原始语料词袋表示的基础上创建一个双包装器（Double Wrapper）：bow-> tfidf-> lsi    
 ```

通过潜在语义索引（Latent Semantic Indexing）（http://en.wikipedia.org/wiki/Latent_semantic_indexing） 将前面转换得到的TF-IDF语料库转换到潜在的3-D空间（3-D因为在这里设置了num_topics = 3）。 用**`models.LsiModel.show_topics()`**探索这3个潜在的维度代表：

```python
lsi.show_topics()
```

### **1.2 对LSI主题模型的结果进行解释**

给每个文档加上序号，便于索引。根据上述主题及主题词构成，即可发现文档的主题归属：

- 1 商业新知:知识图谱为内核,构建商业创新服务完整生态。

- 2 如何更好利用知识图谱技术做反欺诈? 360金融首席数据科学家沈赟开讲。

- 3 知识管理 | 基于知识图谱的国际知识管理领域可视化分析。

- 4 一文详解达观数据知识图谱技术与应用。

- 5 知识图谱技术落地金融行业的关键四步。

- 6 一文读懂知识图谱的商业应用进程及技术背景。

- 7 海云数据CPO王斌:打造大数据可视分析与AI应用的高科技企业。

- 8 智能产业|《人工智能标准化白皮书2018》带来创新创业新技术标准。

- 9 国家语委重大科研项目“中华经典诗词知识图谱构建技术研究”开题。

- 10 最全知识图谱介绍:关键技术、开放数据集、应用案例汇总

- 11 中译语通Jove Mind知识图谱平台 引领企业智能化发展。

- 12 知识图谱:知识图谱赋能企业数字化转型，为企业升级转型注入新能量。

  

  根据LSI的原理可知，“数据”、“技术”和“金融”都是相关词（并且对第一个主题的方向贡献最大），可大体推测该主题跟数据技术在金融领域的应用有关，那么，文档2、5跟该主题的相关性最大； 根据第二个主题的TOP3主题词（“企业”、“数据”、“商业”）可知，该主题主要谈论企业将数据应用于商业实践，那么，文档1、6、7、10、12跟该主题的相关性最大； 以此类推，第三个主题讨论的主要是（借助数据技术）在商业领域的创新，尤其是金融方面的，那么，文档1、8跟该主题相关性最大。 **由于LSI、LDA等主题模型在本质上属于软聚类（Soft Clustering），也就是说，每个主题上的概率就是文章对于这个主题的隶属度，同一个文档可能夹杂着多个主题，只是对应的各个主题的概率不同罢了。** 上述文档对应的主题应该是混合的，每个主题的概率大小不尽相同，在实际应用中，我们一般找出其中概率最大的一个主题。

```python
for doc in corpus_lsi: # bow->tfidf转换 和 tfidf->lsi转换实际上是在这里即时完成的
    print(doc)
```

TF-IDF和LDA等模型也可以用先前的方法进行本地化存储：

```python
lsi.save(os.path.join(TEMP_FOLDER, 'model.lsi')) 
lsi = models.LsiModel.load(os.path.join(TEMP_FOLDER, 'model.lsi'))
```

## 目前比较流行的**文本转换模型**，它们都可以用gensim的接口实现:

### **2.1 词频-逆向文件频率（Term Frequency \* Inverse Document Frequency）**

TF-IDF模型需要一个基于词袋表示(Bag-of-Words Representation）的训练语料库来进行初始化。 在数据转换期间，它采用向量作为输入并返回另一个具有相同维度的向量，只有在训练语料库中出现较少的特征（出现越少对文档的代表性就越强）才能增加该数值。 因此，TF-IDF模型将整型向量转换为实值向量（比如[(1,2),(0,10),(4,2),(5,1)]转换为[(1,0.002),(0,0.121),(4,0.0401),(5,0.5241)]），且保持维度的数量不变。 值得注意的是，该模型返回的结果还可以将得到的向量归一化为（欧几里得）单位长度，使（词汇权重）值介于0到1之间。

```python
model = models.TfidfModel(corpus, normalize=True)
```

### **2.2 潜在语义索引（LSI）**

LSI将文档从词袋表示（Bag-of-Words Representation，优先选择词袋表示，经大量实验，这样得到的效果较好）或者TF-IDF加权空间转换为较低维度的潜在向量空间。 对于上面的示例语料库，我们只使用了3个潜在维度（Latent Dimensions），但在实际语料库处理中，建议将200-500的目标维度作为“黄金标准”[详情见参考资料1]。

```python
model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)
```

将模型训练得出的主题展示出来：

 ```python
 model.print_topics()
 ```

LSI模型训练的独特之处在于：**我们可以在任何时间对模型进行`再次训练`**，只需提供更多用于训练的语料。 它通过一种称为**`在线训练/学习`**`或`**`增量训练/学习`**`（Online Training/Online Learning或者Incremental Training/Incremental Learning）`的过程来完成，该过程对底层模型(Underlying Model)进行增量更新。 由于这个特性，输入的文档流（Document Stream）甚至可能是无穷无尽的 --- 只要在LSI模型需要语料时再喂进新文档，对训练模型保持持续性的新文档输入。

**增量学习的使用方法说明：**

 ```python
 model.add_documents(another_tfidf_corpus)  # 现在LSI模型已经在tfidf_corpus + 另一个tfidf_corpus上进行训练了
 lsi_vec = model[tfidf_vec]                # 将一个新文档转换到LSI空间，但不影响模型
 model.add_documents(more_documents)      # tfidf_corpus + another_tfidf_corpus + 更多的文档 lsi_vec = model[tfidf_vec]
 ```

### **2.3 随机映射（Random Projections）**

RP算法旨在减少向量空间的维度。 这是一种非常有效的（对内存和CPU友好）方法，通过加入一点随机性来近似表示文档之间的TF-IDF距离。 比较推荐的目标维度最好是几百/几千，具体数值取决于语料库的大小。

```python
model = models.RpModel(corpus_tfidf, num_topics=3)
model.num_topics #主题数有多少？
model.num_terms  #主题模型中参与训练的词汇数有多少？
```

### **2.4 隐狄利克雷分配模型（Latent Dirichlet Allocation, LDA）**

LDA是另一种从词袋表示转变为低维度主题空间的文本数据转换模型。 LDA是LSA（也称为多项PCA）的概率扩展（Probabilistic Extension），因此LDA的主题可以解释为词汇的概率分布。 与LSA一样，这些分布也是从训练语料库中自动推断出来的，不需要人为介入。 文档又被解释为这些主题的（软）混合，这跟LSI和LSA一样。

```python
model = models.LdaModel(corpus, id2word=dictionary, num_topics=3)

#展示主题模型中的各个主体及其主题词分布情况：
model.show_topics()
```

### **2.5 层次狄利克雷过程（Hierarchical Dirichlet Process, HDP）**

HDP是一种非参数贝叶斯方法（跟其他的主题模型不同，它不需要事先确定主题的数量），它对于想偷懒的同学来说，简直是福音！以下是它的接口调用方法:

```python
model = models.HdpModel(corpus, id2word=dictionary)
model.num_topics  # 该模型自动生成 主题数有多少？

展示HDP主题模型中的各个主题及其主题词分布情况：

model.show_topics()  
```

