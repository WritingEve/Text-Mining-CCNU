# 文本挖掘从小白到精通（二）---语料库和词向量空间

文本挖掘中一个重要的概念 ---（文本）向量空间，它是将自然语言转化为机器可识别符号的关键一步，文本相似度、文本聚类、文本分类等实际应用皆以此为基础。

培养码代码的好习惯，设置日志，打印程序运行中的细节，以便调试代码。

```python
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
```

```python
import os
import tempfile
TEMP_FOLDER = tempfile.gettempdir()
print('文件夹"{}" 将被用来存储语料和临时性的字典'.format(TEMP_FOLDER))
```

## **一、从字符串到向量（From Strings to Vectors**）

使用之前关于“知识图谱”报道的标题语料库作为示例：

```python
from gensim import corpora
import jieba
```

* 注释：根据打印出的日志可知，'pattern'没正确安装上，这个库是自然语言处理里一个很棒的库，不过目前没怎么更新了，且对中文的支持不给力

```python
jieba.add_word('知识图谱') #防止“知识图谱”被切错词

docs = ['商业新知:知识图谱为内核,构建商业创新服务完整生态。',
'如何更好利用知识图谱技术做反欺诈? 360金融首席数据科学家沈赟开讲。',
'知识管理 | 基于知识图谱的国际知识管理领域可视化分析。',
'一文详解达观数据知识图谱技术与应用。',
'知识图谱技术落地金融行业的关键四步。',
'一文读懂知识图谱的商业应用进程及技术背景。',
'海云数据CPO王斌:打造大数据可视分析与AI应用的高科技企业。',
'智能产业|《人工智能标准化白皮书2018》带来创新创业新技术标准。',
'国家语委重大科研项目“中华经典诗词知识图谱构建技术研究”开题。',
'最全知识图谱介绍:关键技术、开放数据集、应用案例汇总。',
'中译语通Jove Mind知识图谱平台 引领企业智能化发展。',
'知识图谱:知识图谱赋能企业数字化转型，为企业升级转型注入新能量。']
```

再对文本进行分词，用空格隔开变成字符串，方便进行下一步的处理：

```python
documents = [' '.join(jieba.lcut(i)) for i in docs]
#documents 
```

```python
# 移除常用词以及分词
stoplist = [i.strip() for i in open('datasets/stopwords_zh.txt',encoding='utf-8').readlines()]
texts = [[word for word in document.lower().split() if word not in stoplist]
for document in documents]


# 移除仅出现一次的词汇
from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
	for token in text:
        frequency[token] += 1


texts = [[token for token in text if frequency[token] > 1] for text in texts]


from pprint import pprint  #使打印的格式更齐整
pprint(texts)


dictionary = corpora.Dictionary(texts)
dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))  # 保存字典，以备后续查找之用
print(dictionary)

print(dictionary.token2id)

new_doc = "知识图谱 为 企业 转型 助力"
new_vec = dictionary.doc2bow(new_doc.lower().split())
print(new_vec)  # “为”、“助力”等词汇未出现在字典中，因而被忽略
```

要机器“读懂”自然语言，则需要将其转换为机器可识别的符号，比如"0"和"1"，且这种转换的过程中需要最大限度保留自然语言特有的**语义特征**。一种常见的文本表示方法 --- 称为**词袋模型**，即`Bag-of-Words`

在词袋模型模型下，这种表现方式不考虑文法以及词的顺序。https://baike.baidu.com/item/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B/22776998?fr=aladdin

***垃圾入，垃圾出（garbage in, garbage out）***。

通过***gensim.corpora.dictionary.Dictionary***这个类为处理过的语料库中出现的每个词汇分配一个独一无二的整数ID 。将分词后的文档实际转换为向量。函数doc2bow（）只是计算每个不同词汇的出现次数，将词汇转换为整数词汇id，并将结果作为一个词袋（bag-of-words）--- 一个稀疏向量返回，形式为（ word_id1，word_count1），（ word_id2，word_count2），（ word_id3，word_count3）

 “为”、“转型”、“助力”等词汇在字典中不存在，因此不会出现在稀疏向量中。 词汇计数为0的词汇不会出现在稀疏向量中，并且稀疏向量中将永远不会出现像（3,0）这样的元素。

```python
corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  #保存到本地，以作后用
for c in corpus:
	print(c)
```

带有`id = 10`的向量特征表示文档中出现“企业”一词的次数---在这12个文档中， 只有倒数前2个和和倒数第6个的值为1，其他皆为0

## **二、语料库流（Corpus Streaming） --- 每次仅调用一个文档**

上面的*语料（corpus）*完全驻留在内存中，作为普通的Python列表而存在。手头的语料库中有数千万个文档，要将所有这些文档都存储在RAM中是行不通的，这会消耗大量的计算资源，速度奇慢！ 相反，我们假设文档存储在本地的单个文件中，每行一个文档。 Gensim只要求语料库在需要使用时，能够一次返回一个文档向量:

```python
from smart_open import smart_open
class MyCorpus(object):
	def __iter__(self):
		for line in smart_open('datasets/mycorpus.txt', 'r',encoding='utf-8'):
# 假设每一行一个文档，用jieba进行分词
			yield dictionary.doc2bow(' '.join(jieba.lcut(line)).lower().split())
```

每个文档占用单个文件中一行的假设并不重要; 你可以设计**__iter__**函数以适合你的特定输入格式，比如文档目录、待解析的XML、可访问的网络节点...只需解析你的输入以检索每个文档中的所用词汇，然后通过字典将这些词汇转换为它们对应的整数ID，并在**__iter__**中产生具有**生成器**属性的稀疏向量。

```python
corpus_memory_friendly = MyCorpus()  #不需要将语料载入到内存中!
print(corpus_memory_friendly)
```

`corpus_memory_friendly`是一个对象。要查看其中的向量构成，需要遍历语料库，并打印每个文档向量（一次一个）：

```python
for vector in corpus_memory_friendly:  #每次载入一个文档向量
	print(vector)
```

此时的语料库对内存更友好 --- 一次最多只有一个向量驻留在RAM中，只是速度稍慢。

使用*`mycorpus.txt这个`*文件来创建字典（Dictionary），但不是将整个文件加载到本地内存中。 然后，我们将过滤掉语料中的停用词以及词频为1的词汇，从而得到“净化”后的语料。

**`dictionary.filter_tokens`**（或者**`dictionary.add_document`**）将调用**`dictionary.compactify()`**来删除词汇id序列中的间隙，空置的占位符（""）将会被剔除。

```python
from six import iteritems
from smart_open import smart_open
#收集所有词汇的统计信息
dictionary = corpora.Dictionary(' '.join(jieba.lcut(line)).lower().split() for line in smart_open('datasets/mycorpus.txt', 'r',encoding='utf-8'))

#停用词和低频词（这里指仅出现1次的词汇）的ID集合
stop_ids = [dictionary.token2id[stopword] for stopword in stoplist 
if stopword in dictionary.token2id]
once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]

#真正实施去停用词和低频次的操作
dictionary.filter_tokens(stop_ids + once_ids)
print(dictionary)
```

### **三、语料格式（Corpus Formats）**

存在几种用于将向量空间（Vector Space）语料库（向量序列）**序列化**到本地的文件格式。 *Gensim*通过前面提到的*流式语料库接口（Streaming Corpus Interface）*实现：以**`惰性方式（A Lazy Fashion）`**从本地读取大量语料，一次一个文档，而不是一次性将整个语料库读入本地内存中，这在语料库极为庞大时是很折腾电脑的。其中，一种比较值得注意的文件格式是**`Matrix Market`**`格式`（http://math.nist.gov/MatrixMarket/formats.html）。

将文档以Matrix Market格式保存： 

```python
#创建一个包含2个文档的微小语料，以一个python列表呈现

corpus = [[(1, 0.5)], []]  # 其中一个文档故意搞成空的

corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.mm'), corpus)
```

其他的存储格式还有`Joachim’s SVMlight format`(http://svmlight.joachims.org/)、 `Blei’s LDA-C format`(http://www.cs.columbia.edu/~blei/lda-c/) 和 **`GibbsLDA++ format`**(http://gibbslda.sourceforge.net/)。

```python
corpora.SvmLightCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.svmlight'), corpus)
corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus)
corpora.LowCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.low'), corpus)
```

从Matrix Market文件加载**语料库迭代器（Corpus Iterator）**：

```python
corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'corpus.mm'))
```

语料库对象是流式的(Streams)，因此，我们通常无法直接打印它们，只有通过遍历才能看到其中的元素.

通过迭代器列表化来查看语料库中的元素：

```python
# 一种打印语料库的方式是 --- 将其整个载入内存中
print(list(corpus))  #  调用 list() 能将任何序列转化为普通的Python list


# 另一种方法：一次打印一个文档
for doc in corpus:
	print(doc)
```

第二种方式对内存更友好，但是出于测试和开发的目的，调用`list（corpus）`更简单、快捷

接下来，以Blei的LDA-C格式保存相同的Matrix Market文档流：

```python
corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus)
```

通过这种方式，*gensim*也可以用作内存高效的*I / O格式转换工具*：只需使用一种格式加载文档流，然后立即以另一种格式进行保存。 

## **四、与NumPy、SciPy的兼容性**

Gensim还囊括许多*高效且实用的函数*，可以在（http://radimrehurek.com/gensim/matutils.html） 看到，通过这些函数，我们可以轻松的进行`numpy矩阵`的转换：

```python
import gensim
import numpy as np
numpy_matrix = np.random.randint(10, size=[5,2])

numpy_matrix

corpus = gensim.matutils.Dense2Corpus(numpy_matrix)
numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)

#numpy_matrix_dense
```

与`scipy.sparse矩阵`相互转换：

```python
import scipy.sparse
scipy_sparse_matrix = scipy.sparse.random(5,2)

#scipy_sparse_matrix

corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)

#corpus

scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)

#scipy_csc_matrix
```

要获得完整的参考（想要将字典的规模精简下以节约内存？优化语料库和NumPy / SciPy数组之间的转换？），请参阅gensim的API文档。
