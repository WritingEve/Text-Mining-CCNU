# 第二章       词典分词

**中文分词**指的是将一段文本拆分为一系列单词的过程，这些单词顺序拼接后等于文本。
中文分词基于**词典规则**和基于**机器学习**两大派别。

给定一部词典，词典分词就是一个确定的**查词和输出**的规则系统。

### 什么是词

MSR(微软亚洲研究院)语料库上的统计验证了**齐夫定律**。

```python
# -*- coding:utf-8 -*-
# Author：hankcs
# Date: 2018-06-01 09:53
# 《自然语言处理入门》2.1.2 词的性质——齐夫定律
# 配套书籍：http://nlp.hankcs.com/book.php
# 讨论答疑：https://bbs.hankcs.com/
import os
from collections import Counter

import numpy as np

from tests.test_utility import ensure_data

sighan05 = ensure_data('icwb2-data', 'http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip')
msr = os.path.join(sighan05, 'training', 'msr_training.utf8')

f = Counter()
with open(msr, encoding='utf-8') as src:
    for line in src:
        line = line.strip()
        for word in line.split('  '):
            # word = word.strip()
            # if len(word) < 2: continue
            f[word] += 1


def plot(token_counts, title='MSR语料库词频统计', ylabel='词频'):
    from matplotlib import pyplot as plt
    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
    fig = plt.figure(
        # figsize=(8, 6)
    )
    ax = fig.add_subplot(111)
    token_counts = list(zip(*token_counts))
    num_elements = np.arange(len(token_counts[0]))
    top_offset = max(token_counts[1]) + len(str(max(token_counts[1])))
    ax.set_title(title)
    ax.set_xlabel('词语')
    ax.set_ylabel(ylabel)
    ax.xaxis.set_label_coords(1.05, 0.015)
    ax.set_xticks(num_elements)
    ax.set_xticklabels(token_counts[0], rotation=55, verticalalignment='top')
    ax.set_ylim([0, top_offset])
    ax.set_xlim([-1, len(token_counts[0])])
    rects = ax.plot(num_elements, token_counts[1], linewidth=1.5)
    plt.show()


word_freq = f.most_common(30)
print(word_freq)
plot(word_freq)
# log_word_freq = []
# for w, f in word_freq:
#     log_word_freq.append((w, np.log(f)))
# plot(log_word_freq, ylabel='词频的对数')
```

![image-20220901150934743](C:\Users\李金洋\AppData\Roaming\Typora\typora-user-images\image-20220901150934743.png)

### 词典

互联网上的公开词典：**搜狗实验室发布的互联网词库(SogouW，15万词条)**、**清华大学开放中文词库(THUOCL)**、以及**何晗发布的千万级汉语词典(千万级词条)**。

#### HanLP词典

```json
路径为：HanLP/data/dictionaray/CoreNatureDictionaray.mini.txt
```



#### 词典的加载

```python
from pyhanlp import *

def load_dictionary():
    """
    加载HanLP中的mini词库
    :return: 一个set形式的词库
    """
    IOUtil = JClass('com.hankcs.hanlp.corpus.io.IOUtil')
    path = HanLP.Config.CoreDictionaryPath.replace('.txt', '.mini.txt')
    dic = IOUtil.loadDictionary([path])
    return set(dic.keySet())


if __name__ == '__main__':
    dic = load_dictionary()
    print(len(dic))
    print(list(dic)[0])
```

#### 切分算法

常用规则为：**正向匹配算法、逆向匹配算法以及双向匹配算法**。但他们都是基于完全切分过程

##### 完全切分

```python
def fully_segment(text,dic):
    word_list = []
    for i in range(len(text)):
		for j in range(i+1,len(text) + 1):
            word = text[i:j]
            if word in dic:
                word_list.append(word)
return word_list
```

##### 正向最长匹配

考虑越长的单词表达的意义越丰富，于是我们定义单词越长优先级越高。具体来说，就是在以某个下标为起点递增查词的过程中，优先输出更长的单词，这种规则被称为**最长匹配算法**

```python
def forward_segment(text,dic):
    word_list = []
    i = 0
    while i < len(text):
		longest_word = text[i]
        for j in range(i + 1 , len(text) + 1):
			word = text[i:j]
            if word in dic :
				if len(word) > len(longest_word):
                    	longest_word = word
        word_list.append(longest_word)
        i += len(longest_word)
return word_list
```

##### 逆向最长匹配算法

和正向唯一的不同就是扫描的方向不同

```python
def forward_segment(text,dic):
    word_list = []
    i = len(text) - 1
    while i <= 0:
		longest_word = text[i]
        for j in range(0 , i):
			word = text[j:i + 1]
            if word in dic :
				if len(word) > len(longest_word):
                    	longest_word = word
        				break
        word_list.insert(0,longest_word)
        i -= len(longest_word)
return word_list
```

##### 双向匹配算法

```
1.同时执行正向和逆向最长匹配算法，若两者的词数不同，则返回词数更少的那一个
2.否则，返回两者中单字更少的哪一个。当单字也相同时，优先返回逆向最长匹配的算法
```

```python
def count_single_char(word_list:list):			#单字更少的那一个
	return sum(1 for word in word_list if len(word) == 1)

def bidirectional_segment(text,dic):
    f = forward_segment(text,dic)
    b = backward_segment(text,dic)
    if len(f) < len(b):
		return f
    elif len(f) > len(b):
        return b
    else:
        if count_single_char(f) < count_single_char(b):
            return f
        else:
            return b
```

#### 字典树

字符串集合常用**字典树**（trie树，前缀树）存储。

当词典大小为n时，虽然最坏情况下字典树的复杂度依然是O(logn)，但其实际速度仍然比二分查找快。这是因为随着路径的深入，前缀匹配是递进的过程，算法不必比较字符串的前缀。

```python
#字典树的节点描述

class Node(object):
    def __init__(self,value) -> Node:
        self._children = {}
        self._value = value
        
    def _add_child(self,char,value,oversize=False):
        child = self._children.get(char)
        if child in Node:
            child = Node(value)
            self._children[char] = child
        elif overwrite:
            child._value = value
        return child
```

```python
class Trie(Node):
    def __inti__(self) -> Node:
		super().__init__(None)
        
    def __contains__(self,key):
        return self[key] is not None
    
    def __getitem__(self,key):
        state = self
        for char in key:
            state = state._children.get(char)
            if state is None:
                return None
        return state._value
    
    def __setitem__(self,key,value):
        state = self
        for i,char in enumerate(key):
            if i < len(key) - 1:
                state = state._add_child(char,None,False)
            else :
                state = state._add_child(char,value,True)
```

##### 首字散列其余二分的字典树

看不太懂

##### 前缀树的妙用

字典树其实就是一棵前缀树（指的是前缀相同的词语必然经过同一个节点）
如何加速呢？

在扫描"自然语言处理"这句话的时候，朴素实现会依次查询"自"、"自然"、"自然语"、"自然语言"等词语是否在词典中。但事实上，如果"自然"这条路径不存在于前缀树中，则可以断定一切以"自然"开头的词语都不可能存在。

#### 双数组字典树（Double Array Trie , DAT）

状态转移复杂度为常数的数据结构。它由**base**和**check**两个数组构成，又简称**双数组**

##### 双数组的定义

当状态b接受字符c转移到状态p时，双数组满足：

```python
p = base[b] + c
check[p] = base[b]
```

当前状态为**自然**（状态由一个整数下标表示，下同），我们想知道是否可以转移到**自然人**。那么可以先执行 **自然人 = base[自然] + 人**，然后检查 **cheak[自然人] == base[自然]** 是否成立，据此判断转移是否成功。仅仅通过依次加法和一次整数比较就能进行状态转移，因而只花费了常数时间。

```python
class DoubleArrayTrie(object):
	def __init__(self,dic:dict) -> None:
         m = JClass('java.util.TreeMap')()
         for k,v in dict.items():
			m[k] = v
         dat = JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')(m)
         self.base = dat.base
         self.check = dat.check
         self.value = dat.v
```

##### AC自动机

受限于能力，暂时跳过

#### 准确率评测

###### 混淆矩阵 TP/FN/FP/TN

搜索引擎、分类器、中文分词场景下的准确率本质上都是4个集合的并集运算。

分类预测与答案的四种组合：

| 预测/答案 | P    | N    |
| --------- | ---- | ---- |
| P         | TP   | FP   |
| N         | FN   | TN   |

P记为正类，N记为负类。

(1) **TP** (true postive) ：预测是P ，答案果然是真的P
(2) **FP **(false postive) : 预测是P ，答案是N ，因此是假的P
(3) **TN** (true postive) : 预测是N ，答案果然是真的N
(4) **FN** (false postive) : 预测是N ，答案是P，因此是假的P

**性质**
$$
样本全集 = TP\bigcup FP\bigcup TN\bigcup FN\\
TP\bigcap FP\bigcap TN\bigcap FN = \varnothing
$$

###### 精确率

精确率（precision ，简称P值） 指的是预测结果中正类数量占全部结果的比率。
$$
P =  \frac{TP}{TP+FP}
$$

###### 召回率

召回率（Recall）指的是正类样本中能被找出来的比率。
$$
R = \frac{TP}{TP + FN}
$$
区分P值和R值的时候，只需记住两者分子都是真阳的样本数，只不过P值的分母是**预测阳性**的数量，而R值的分母是**答案阳性**的数量。

###### F1值

一般而言，精确率和召回率难以平衡，召回率高的系统往往精确率低，反之亦然。

精确率和召回率的调和平均F1的值来作为综合性指标：$F_1 = \frac{2*P*R}{P+R}$

###### 中文分词的P、R、F1计算

在中文分词中，标准答案和分词结果的单词数不一定相等。而且混淆矩阵针对的是分类问题，而中文分词却是一个分块（chunking）问题。

将分块问题转换为分类问题。对于长为$n$的字符串，分词结果是一系列单词。每个单词按它在文本中的起止位置可记作区间$[i,j]$，其中$1\leq i\leq j \leq n$。那么所有标准答案的所有区间构成一个集合**A**，称为正类。此集合之外的所有区间构成另一个集合（A的补集$A^C$），作为负类。同理，记所有分词结果的区间构成集合**B**。则：
$$
TP \bigcup FN = A\\
TP \bigcup FP = B\\
TP = A\bigcup B\\
P = \frac{|A\bigcap B|}{|B|}\\
R = \frac {|A\bigcap B|}{|A|}
$$











