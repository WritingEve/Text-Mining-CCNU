# 文本挖掘从小白到精通（五）---主题模型的主题数确定和可视化

本文将回到主题模型这个话题中来，聊聊主题模型中的主题数该如何确定，以及主题模型的可视化

首先，导入必要的库：

```python
from gensim.corpora import Dictionary
from gensim.models import ldamodel
from gensim.models import CoherenceModel, LdaModel
from gensim import models
import numpy
# %matplotlib inline
```

在这里，展示gensim的主题模型中的2个新的方法 --- `get_term_topics`和`get_document_topics`，接下来，大家将会看到，在不同的语境中，同一个词汇的意义会不一样的情形（The same word which might have different meanings in different context）。

以“苹果”一词为例，苹果最常见的含义是水果；另一个含义是苹果公司，该公司拥有世界知名的iPhone和Mac。

构建了如下语料库，已经经过分词和去停用词处理:

```python
texts = [
            ['苹果','叶子','椭圆形','树上'],
            ['植物','叶子','绿色','落叶乔木'],
            ['水果','苹果','红彤彤','味道'],
            ['苹果','落叶乔木','树上','水果'],
            ['植物','营养','水果','维生素'],
            ['营养','维生素','苹果','成分'],
            ['互联网','电脑','智能手机','高科技'],
            ['苹果','公司','互联网','品质'],
            ['乔布斯','苹果','硅谷'],
            ['电脑','智能手机','苹果','乔布斯'], 
            ['苹果','电脑','品质','生意'],
            ['电脑','品质','乔布斯'],
            ['苹果','公司','生意','硅谷']

            ]

dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
```

接下来将训练两个主题模型，差异在于主题数的不同，按照构建的语料库构成来看，主题数应该是2，假如是其他的主题数，模型的效果应该不好。

基于假设，“好”的主题模型的主题数为2，“坏”的主题模型的主题数为6。

```python
numpy.random.seed(1) # 设置随即种子数，以便相同的设置能跑出相同的结果，可复现
goodLdaModel = LdaModel(corpus=corpus, id2word=dictionary,
     iterations=50, num_topics=2)
badLdaModel = LdaModel(corpus=corpus, id2word=dictionary, 
     iterations=50, num_topics=6)
```

通过CoherenceModel这个类中的两个指标 --- U_Mass Coherence和C_V coherence来判定主题模型质量的好坏（对文本的主题区分度效果，即能将混沌的语料切分出人类可理解的主题），这两个指标都是数值越大，主题模型的效果越好。

## **1 如何通过指标确定合理的主题数**

### **1.1 使用U_Mass Coherence**

```python
goodcm = CoherenceModel(model=goodLdaModel, corpus=corpus, 
       dictionary=dictionary, coherence='u_mass')
badcm = CoherenceModel(model=badLdaModel, corpus=corpus,
       dictionary=dictionary, coherence='u_mass')
       
print(goodcm.get_coherence())
print(badcm.get_coherence())
```

 **1.2 使用 C_V coherence**

```python
goodcm = CoherenceModel(model=goodLdaModel, texts=texts, 
    dictionary=dictionary,  coherence='c_v')
badcm = CoherenceModel(model=badLdaModel, texts=texts,
    dictionary=dictionary,  coherence='c_v')
    
print(goodcm.get_coherence())
print(badcm.get_coherence())
```

以上结果中，虽然数值差异不大，但“好”的主题模型的C_V coherence值要大于“坏”的主题模型的C_V cohere**nce值。**

由此，我们即可知道主题数应该设置为2，以此期望主题模型能体现“苹果”一词的多义性---一个跟（苹果）公司有关，一个跟水果有关。

```python
model = ldamodel.LdaModel(corpus, id2word=dictionary,
           iterations=500,num_topics=2,alpha='auto')
          
#展示两个主题中的主题词，由之推断出主题的大致内容：
model.show_topics(num_words=5)
```

结果展示中，“苹果”是这两个主题中最有影响力的词汇（主题权重值在两个主题中都是最大，分别为0.143和0.135）。两个主题中除“苹果”以外的词汇相当于“语境”，有助于我们分辨各个主题下的“苹果”的具体含义，是指水果还是指乔帮主所创立的公司。接下来，对主题模型跑出来的结果进行深入分析，试试 `get_term_topics`和`get_document_topics`这两个新方法。

### **2 预测词汇的主题归属**

函数`get_term_topics`返回属于某个词汇属于特定主题的几率（the odds of that particular word belonging to a particular topic）。

举3个个例子：

```python
'''
[(0,
  '0.104*"苹果" + 0.077*"水果" + 0.073*"植物" + 0.071*"落叶乔木" + 0.070*"叶子" + 0.065*"树上"'),
 (1,
  '0.175*"苹果" + 0.084*"电脑" + 0.081*"品质" + 0.062*"乔布斯" + 0.061*"生意" + 0.060*"公司"')]
 '''

#根据主题模型运行出来的结果，序号为0的暂定为“水果”，序号为1的暂定为“公司”，用来测试几个词汇的主题归属情况
topic_list = ['水果','公司']

[(topic_list[i[0]],i[1]) for i in model.get_term_topics('树上')]
```

结果显示“树上”这个词汇更加靠近“水果”这个主题。

```python
[(topic_list[i[0]],i[1]) for i in model.get_term_topics('智能手机')]
[(topic_list[i[0]],i[1]) for i in model.get_term_topics('落叶乔木')]
```

### **3 预测文档的主题归属**

`get_document_topics`是一个用于推断文档主题归属的函数/方法，在这里，假设一个文档可能同时包含若干个主题，但在每个主题上的概率不一样，文档最有可能从属于概率最大的主题。该函数也可以让我们了解某个文档中的某个词汇在主题上的分布情况。

当参数`per_word_topics`设置为True时，`get_document_topics`方法返回词汇ID（在词典中的位置），以及最可能的主题id的列表（按降序排列，可能性最大的排在前面）。

```python
bow_fruit = ['苹果','树上','落叶乔木','苹果']
bow_company = ['苹果','电脑','乔布斯']

bow = model.id2word.doc2bow(bow_fruit)     # 现将文档转换为词袋表示
doc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True)

print(word_topics)
```

结果显示：[(1, [0, 1]), (3, [0, 1]), (6, [0, 1])]

1、3、6对应文档bow_fruit中的3个词汇：'苹果'、'树上'、'落叶乔木'，它们的主题更加倾向于是“水果”，因为每个词汇的主题序号中，0都排在靠前的位置，也就是“水果”这个主题更为明显。

`get_document_topics`这个方法产生了3个值doc_topics、 word_topics和 phi_values。 对于phi_values而言，它包含特定词汇在各主题上的phi值，且按特征长度缩放。 Phi本质上是文档中某个词汇从属于某个主题的概率值。

```python
print(phi_values)
```

结果其实跟word_topics的结果的结果差不多，除了各个主体序号后多出的数值---phi_values，有了这个数值，我们能判断词汇从属于特定主题的程度如何。

对第二个文档`bow_company`做同样的事情。

```python
bow = model.id2word.doc2bow(bow_company)  # 现将文档转换为词袋表示
doc_topics, word_topics, phi_values = model.get_document_topics(bow, per_word_topics=True)

print(word_topics)
```

基于不同的上下文，同一个词汇会归属到不同的主题下。 这与我们之前的`get_term_topics方法`不同，它是一个'**静态（Static）**'的**主题分布（Topic Distribution）**。

因为基于gensim实现的LDA使用**变分贝叶斯采样（Variational Bayes Sampling）**，所以某个文档中出现多次的某个词汇的仅给出一个主题分布。例如，对于句子“苹果长在树上，秋天上面会结满红彤彤的苹果”，其中的两个“苹果”都会被分配到主题“topic_0（水果）”上，也就是说，这两个“苹果”具有相同的主题分布。

接下来，通过get_document_topics来获取语料库中所有文档的“doc_topics”，“word_topics”和“phi_values”：

```python
all_topics = model.get_document_topics(corpus, per_word_topics=True)

cnt = 0
for doc_topics, word_topics, phi_values in all_topics:
    print('新文档:{} \n'.format(cnt),texts[cnt])
    doc_topics = [(topic_list[i[0]],i[1]) for i in doc_topics]
    word_topics = [(dictionary.id2token [i[0]],i[1]) for i in word_topics]
    phi_values = [(dictionary.id2token [i[0]],i[1]) for i in phi_values ]
    print('文档主题:', doc_topics)
    print('词汇主题:', word_topics)
    print('Phi值:', phi_values)
    print(" ")
    print('-------------- \n')
    cnt+=1
```

如果想在变量中存储语料库中所有文档的“doc_topics”、“word_topics”和“phi_values”，以及后续使用这3个数据来获取特定文档的详细信息，可以按以下方式实现:

```python
topics = model.get_document_topics(corpus, per_word_topics=True)
all_topics = [(doc_topics, word_topics, word_phis) for doc_topics, word_topics, word_phis in topics]
```

现在，我可以访问特定文档的详细信息，如文档['苹果', '落叶乔木', '树上', '水果']，结果如下所示：

```python
doc_topic, word_topics, phi_values = all_topics[2]
print('文档主题:', doc_topics, "\n")
print('词汇主题:', word_topics, "\n")
print('Phi值:', phi_values)
```

也可以通过以下方式打印所有文档的详细信息：

```python
cnt = 0
for doc in all_topics:
  print('新文档:{} \n'.format(cnt),texts[cnt])
  print('文档主题:', doc[0])
  print('词汇主题:', doc[1])
  print('Phi值:', doc[2])
  print(" ")
  print('-------------- \n')
  cnt+=1
```

## **4 对特定的主题词“着色”**

如果想给语料库中的某个词汇着色，那么`get_term_topics`将是最好的选择。 如果没有，用`get_document_topics`也行。现在尝试使用`matplotlib`对某些词汇进行着色。 这只是绘制单词的一种方式 - 有更多更好的方法。 比如，[WordCloud]（https://github.com/amueller/word_cloud） 就是这样一个实现词汇可视化的python包。

为简单起见，将`topic_1`设定为红色，将`topic_0`设定为蓝色:

```python
#  以下是一个给词汇着色的函数。 如前所述，有很多方法可以做到这一点。
import matplotlib.pyplot as plt
import matplotlib.patches as patches
def color_words(model, doc):  
  # 将文档转化为词袋表示
  doc = model.id2word.doc2bow(doc)
  # 获取词汇的主题分布
  doc_topics, word_topics, phi_values = model.get_document_topics(doc, per_word_topics=True)

  # 颜色 - 主题匹配
  topic_colors = { 1:'red', 0:'blue'}
    
  # 设置画图的背景
  fig = plt.figure()
  ax = fig.add_axes([0,0,1,1])

  # 一个“暗黑”小技巧，使词汇之间的间距保持在合理的范围之内
  word_pos = 1/len(doc)
    
 # 使用matplotlib对词汇进行绘制
 for word, topics in word_topics:
   ax.text(word_pos,
                0.8,
                model.id2word[word],
                horizontalalignment='center',
                verticalalignment='center',
                fontsize=20, 
                color=topic_colors[topics[0]],  # 选择可能性最大的主题
                transform=ax.transAxes)
        word_pos += 0.2
    ax.set_axis_off()
    plt.show()
```

现在开始对一些文档进行可视化展示，试图发现词汇和主题的关联性：

```python
# 对“水果”主题的文档进行可视化展示

import matplotlib.pyplot as plt

%matplotlib inline
plt.rcParams['font.sans-serif']=['SimHei']  #用来正常显示中文标签

bow_fruit = ['水果','苹果','树上']

color_words(model, bow_fruit)

bow_company = ['苹果','电脑','乔布斯','智能手机']
color_words(model, bow_company)

#  现在找一个主题分布较为均匀的语句，看看这里的可视化效果如何
 
doc = ['苹果','电脑','苹果','树上','乔布斯','智能手机','硅谷','植物']
color_words(model, doc)
```

这里，我们看到文档的词汇着色基本我们预期的（“植物”没分对）。:)

再对整个词典（dictionary）里的词汇进行着色。

在这里，与上面使用`get_document_topics`不同的是，我们使用`get_term_topics`这个方法，对整个字典（dictionary）里的词汇进行颜色区分，蓝色的代表主题0 - “水果”，红色代表主题1 - “公司”。

```python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
def color_words_dict(model, dictionary):
  word_topics = []
  for word_id in dictionary:
    word = str(dictionary[word_id])
    # get_term_topics方法返回静态的主题
    probs = model.get_term_topics(word)
       # 我们正在创建词汇的主题分布，它与get_document_topics所结果创建的类似
    try:
       if probs[0][1] >= probs[1][1]:
          word_topics.append((word_id, [0, 1]))
       else:
          word_topics.append((word_id, [1, 0]))
        # 在这种情况下，只会返回一个主题（概率值最大的那个主题）
   except IndexError:
     word_topics.append((word_id, [probs[0][0]]))
            
    # 颜色 - 主题匹配
    topic_colors = { 1:'red', 0:'blue'}
    
     # 设置画图的背景
    fig = plt.figure()
    ax = fig.add_axes([0,0,1,1])

    # 一个“暗黑”小技巧，使词汇之间的间距保持在合理的范围之内
    word_pos = 1/len(doc)
         
    #使用matplotlib对词汇进行绘制
    for word, topics in word_topics:
      ax.text(word_pos, 0.8, model.id2word[word],
            horizontalalignment='center',
            verticalalignment='center',
            fontsize=20, 
            color=topic_colors[topics[0]],  # 选择可能性最大的主题
            transform=ax.transAxes)
      word_pos += 0.2  

    ax.set_axis_off()
    plt.show()
    
    color_words_dict(model, dictionary)
```

大部分红色词汇与“公司”这个主题有关，蓝色词语与“水果”主题有关。还可以注意到某些单词（如“叶子”、“树上”、“互联网”、“智能手机”等词汇）似乎颜色不正确（也就是主题没划分对）。 

**LDA模型的使用场景及需要注意的问题：**

- 主题模型需要大量的文本数据
- 适合长文本，比如文章的正文数据，不适合微博、评论这样的短文本数据
- 小语料库意味着LDA算法可能不会为每个单词分配较为理想的主题比例
- 微调模型的参数并收集更多的语料库将提升模型表现，最终改善词汇着色的结果。

**5 主题模型可视化**

pyLDAvis是python中的一个对LDA主题模型进行交互可视化的库，它可以将主题模型建模后的结果，制作成一个网页交互版的结果分析工具。

pyLDAvis在可视化呈现中着重回答如下三个问题：

- What is the meaning of each topic? 每个主题的意义是什么？

- How prevalent is each topic? 每个主题在总语料库的比重如何？

- How do the topics relate to each other? 主题之间有什么关联？

  ```python
  import warnings
  try:
    import pyLDAvis.gensim
    CAN_VISUALIZE = True
    pyLDAvis.enable_notebook()
    from IPython.display import display
  except ImportError:
      ValueError("SKIP: please install pyLDAvis")
      CAN_VISUALIZE = False
  warnings.filterwarnings('ignore')  # 忽视所有的提示
  %matplotlib inline
  
  prepared = pyLDAvis.gensim.prepare(model, corpus, dictionary)
  pyLDAvis.show(prepared,open_browser=True)
  ```

  显示完日志后，在web浏览器上就会弹出新的标签页，出现可交互的可视化页面。对于可视化结果的解读，可根据上述pyLDAvis着重分析的三个维度来展开。